{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pformat\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "\n",
    "from mld.callback import ProgressLogger\n",
    "from mld.config import parse_args\n",
    "from mld.data.get_data import get_datasets\n",
    "from mld.models.get_model import get_model\n",
    "from mld.utils.logger import create_logger\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg.TRAIN.BATCH_SIZE: 64\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e023bbff01b4e29ba73ffd4ffb82f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 196, 263])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv = [\"train.py\", \"--cfg\", \"configs/config_vae_humanml3d.yaml\", \"--cfg_assets\", \"configs/assets.yaml\", \"--batch_size\", \"64\", \"--nodebug\"]\n",
    "cfg = parse_args()  # parse config file\n",
    "logger = create_logger(cfg, phase=\"train\")\n",
    "datasets = get_datasets(cfg, logger=logger)\n",
    "model = get_model(cfg, datasets[0])\n",
    "data_loader = datasets[0].train_dataloader()\n",
    "\n",
    "batch = next(iter(data_loader))\n",
    "batch.keys()\n",
    "batch['motion'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7403, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.allsplit_step(split='train', batch=batch, batch_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: torch.Size([1, 64, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 196, 22, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from: mld/models/modeltype/mld.py: MLD:train_vae_forward\n",
    "feats_ref = batch[\"motion\"]\n",
    "lengths = batch[\"length\"]\n",
    "motion_z, dist_m = model.vae.encode(feats_ref, lengths)\n",
    "feats_rst = model.vae.decode(motion_z, lengths)\n",
    "joints_rst = model.feats2joints(feats_rst)\n",
    "joints_rst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 256]),\n",
       " tensor([[[0.7725, 1.0069, 0.4456,  ..., 0.5810, 2.2325, 1.2504],\n",
       "          [0.7822, 1.2683, 0.7755,  ..., 0.4844, 2.7962, 1.2504],\n",
       "          [0.5818, 1.8936, 0.8240,  ..., 0.9396, 2.7249, 0.9010],\n",
       "          ...,\n",
       "          [0.4743, 0.9068, 0.6290,  ..., 0.7586, 3.6105, 1.5674],\n",
       "          [1.1121, 0.7113, 1.5482,  ..., 0.5164, 1.3498, 2.7461],\n",
       "          [0.9881, 0.9536, 0.5386,  ..., 0.4572, 4.2650, 2.4904]]],\n",
       "        grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_z.shape, dist_m.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.is_vae"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff31f150cab4250bd25cb0ab70dc5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    }
   ],
   "source": [
    "sys.argv = [\"train.py\", \"--cfg\", \"configs/config_mld_humanml3d.yaml\", \"--cfg_assets\", \"configs/assets.yaml\", \"--batch_size\", \"64\", \"--nodebug\"]\n",
    "# python -m train --cfg configs/config_mld_humanml3d.yaml --cfg_assets configs/assets.yaml --batch_size 64 --nodebug\n",
    "cfg = parse_args()  # parse config file\n",
    "logger = create_logger(cfg, phase=\"train\")\n",
    "datasets = get_datasets(cfg, logger=logger)\n",
    "model = get_model(cfg, datasets[0])\n",
    "data_loader = datasets[0].train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 196, 263])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "batch.keys()\n",
    "batch['motion'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise_pred: torch.Size([64, 1, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.9939, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.allsplit_step(split='train', batch=batch, batch_idx=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "from builtins import ValueError\n",
    "from multiprocessing.sharedctypes import Value\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "# from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mld.config import parse_args\n",
    "# from mld.datasets.get_dataset import get_datasets\n",
    "from mld.data.get_data import get_datasets\n",
    "from mld.data.sampling import subsample, upsample\n",
    "from mld.models.get_model import get_model\n",
    "from mld.utils.logger import create_logger\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length-50: a man kicks with something or someone with his left leg.\n",
      "Length-100: A person is skipping rope.\n",
      "Length-100: a person walks backward slowly.\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv = [\"demo.py\", \"--cfg\", \"configs/config_mld_humanml3d.yaml\", \"--cfg_assets\", \"configs/assets.yaml\", \"--example\", \"./demo/example.txt\"]\n",
    "cfg = parse_args(phase=\"demo\")\n",
    "cfg.FOLDER = cfg.TEST.FOLDER\n",
    "cfg.Name = \"demo--\" + cfg.NAME\n",
    "logger = create_logger(cfg, phase=\"demo\")\n",
    "from mld.utils.demo_utils import load_example_input\n",
    "\n",
    "text, length = load_example_input(cfg.DEMO.EXAMPLE)\n",
    "task = \"Example\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(\n",
    "            str(x) for x in cfg.DEVICE)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "dataset = get_datasets(cfg, logger=logger, phase=\"test\")[0]\n",
    "\n",
    "# create mld model\n",
    "total_time = time.time()\n",
    "model = get_model(cfg, dataset)\n",
    "\n",
    "state_dict = torch.load(cfg.TEST.CHECKPOINTS,\n",
    "                            map_location=\"cpu\")[\"state_dict\"]\n",
    "\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "model.sample_mean = cfg.TEST.MEAN\n",
    "model.fact = cfg.TEST.FACT\n",
    "model.to(device)\n",
    "model.eval()\n",
    "''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mld_time = time.time()\n",
    "with torch.no_grad():\n",
    "    rep_lst = []    \n",
    "    rep_ref_lst = []\n",
    "    texts_lst = []\n",
    "    # task: input or Example\n",
    "    if text:\n",
    "        # prepare batch data\n",
    "        batch = {\"length\": length, \"text\": text}\n",
    "        \n",
    "        for rep in range(cfg.DEMO.REPLICATION):\n",
    "            # text motion transfer\n",
    "            if cfg.DEMO.MOTION_TRANSFER:\n",
    "                joints = model.forward_motion_style_transfer(batch)\n",
    "            # text to motion synthesis\n",
    "            else:\n",
    "                joints = model(batch)\n",
    "\n",
    "            # cal inference time\n",
    "            infer_time = time.time() - mld_time\n",
    "            num_batch = 1\n",
    "            num_all_frame = sum(batch[\"length\"])\n",
    "            num_ave_frame = sum(batch[\"length\"]) / len(batch[\"length\"])\n",
    "\n",
    "            # upscaling to compare with other methods\n",
    "            # joints = upsample(joints, cfg.DATASET.KIT.FRAME_RATE, cfg.DEMO.FRAME_RATE)\n",
    "            nsample = len(joints)\n",
    "            id = 0\n",
    "            for i in range(nsample):\n",
    "                joints[i].detach().cpu().numpy()\n",
    "            if cfg.DEMO.OUTALL:\n",
    "                rep_lst.append(joints)\n",
    "                texts_lst.append(batch[\"text\"])\n",
    "total_time = time.time() - total_time\n",
    "print(\n",
    "    f'Total time spent: {total_time:.2f} seconds (including model loading time and exporting time).'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 models:\n",
    "model.denoiser\n",
    "model.vae.decoder\n",
    "''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DELETE ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/mld/1222_PELearn_Diff_Latent1_MEncDec49_MdiffEnc49_bs64_clip_uncond75_01'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.denoiser.ablation_skip_connection\n",
    "# model.denoiser.query_pos.pe.shape\n",
    "model.vae.arch\n",
    "model.cfg.FOLDER_EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = torch.nn.TransformerEncoderLayer(\n",
    "            d_model=256,\n",
    "            nhead=4,\n",
    "            dim_feedforward=1024,\n",
    "            activation='gelu')\n",
    "encoder = torch.nn.TransformerEncoder(encoder_layer,\n",
    "                                        num_layers=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.rand(3, 6, 256)\n",
    "_ = encoder(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
